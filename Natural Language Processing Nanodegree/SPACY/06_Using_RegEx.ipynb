{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The Strengths of RegEx\n",
        "\n",
        "There are several strengths to RegEx.\n",
        "\n",
        "- Due to its complex syntax, it can allow for programmers to write robust rules in short spaces.\n",
        "- It can allow the researcher to find all types of variance in strings\n",
        "- It can perform remarkably quickly when compared to other methods.\n",
        "- It is universally supported\n"
      ],
      "metadata": {
        "id": "_NiVHRtzH8d1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## he Weaknesses of RegEx\n",
        "\n",
        "Despite these strengths, there are a few weaknesses to RegEx.\n",
        "\n",
        "-  syntax is quite difficult for beginners. (I still find myself looking up how to do certain things).\n",
        "- It order to work well, it requires a domain-expert to work alongside the programmer to think of all ways a pattern may vary in texts."
      ],
      "metadata": {
        "id": "C645eNLMIElk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ajZOOSIoB4hi"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r\"((\\d){1,2} (January|February|March|April|May|June|July|August|September|October|November|December))\"\n",
        "\n",
        "text = \"This is a date 2 February. Another date would be 14 August.\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YktVo0CMIOXf",
        "outputId": "30b30d86-d5d6-450e-fef2-6c2f9100f6e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('2 February', '2', 'February'), ('14 August', '4', 'August')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice, however, that we have a lot of superfluous information for each match. These are the components of each match. There are several ways we can remove them. One way is to use the command `finditer`, rather than findall in RegEx."
      ],
      "metadata": {
        "id": "A-2NRdYKIvz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_matches = re.finditer(pattern, text)\n",
        "iter_matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxbHsuECIvqp",
        "outputId": "5cfcc9ac-f5fd-4b1c-ca4e-efa7745a3843"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<callable_iterator at 0x7f45b8ad0bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an iterator object, we can loop over it, however, and get our results.\n",
        "iter_matches = re.finditer(pattern, text)\n",
        "for hit in iter_matches:\n",
        "    print(hit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH47WvFDIvn5",
        "outputId": "90a12199-d53e-4224-e330-f7eeb3d1ee36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(15, 25), match='2 February'>\n",
            "<re.Match object; span=(49, 58), match='14 August'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_matches = re.finditer(pattern, text)\n",
        "for hit in iter_matches:\n",
        "    start = hit.start()\n",
        "    end = hit.end()\n",
        "    print(text[start:end])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9BFM2fHJUhL",
        "outputId": "af6d4a97-4a60-4694-e69b-053b0f16393f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 February\n",
            "14 August\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Use RegEx in spaCy\n",
        "\n",
        "Things like dates, times, IP Addresses, etc. that have either consistent or fairly consistent structures are excellent candidates for RegEx. Fortunately, spaCy has easy ways to implement RegEx in three pipes: Matcher, PhraseMatcher, and EntityRuler. One of the major drawbacks to the Matcher and PhraseMatcher, is that they do not align the matches as doc.ents. Because this textbook is about NER and our goal is to store the entities in the doc.ents, we will focus on using RegEx with the EntityRuler. In the next notebook, we will examine other methods.\n",
        "\n",
        "In the previous notebook, we saw how the code below allowed for us to capture the phone number in the string. I have modified it a bit here for reasons that will become a bit more clear below."
      ],
      "metadata": {
        "id": "SreenIeCJfVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the requisite library\n",
        "import spacy\n",
        "\n",
        "#Sample text\n",
        "text = \"This is a sample number 555-5555.\"\n",
        "\n",
        "#Build upon the spaCy Small Model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "#Create the Ruler and Add it\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
        "patterns = [\n",
        "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [\n",
        "                    {\"SHAPE\": \"ddd\"},\n",
        "                    {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
        "                    {\"SHAPE\": \"dddd\"}\n",
        "                ]}\n",
        "            ]\n",
        "#add patterns to ruler\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "#create the doc\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF7OZvJEJYpv",
        "outputId": "5ccd5961-9825-4cdd-f3b8-abe54b527925"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "555-5555 PHONE_NUMBER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let’s write some RegEx to capturee 555-5555.\n",
        "pattern = r\"((\\d){3}-(\\d){4})\"\n",
        "text = \"This is a sample number 555-5555.\"\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP2CsLHxJnRE",
        "outputId": "6710087c-943d-4c73-f225-ce27c53d53c7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('555-5555', '5', '5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay. So, now we know that we have a RegEx pattern that works. Let’s try and implement it in the spaCy EntityRuler. We can do that with the code below. When we execute the code below, we have no output."
      ],
      "metadata": {
        "id": "0jDFBaR3J7VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build upon the spaCy Small Model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "#Create the Ruler and Add it\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
        "patterns = [\n",
        "                {\n",
        "                    \"label\": \"PHONE_NUMBER\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"((\\d){3}-(\\d){4})\"}}]\n",
        "                }\n",
        "            ]\n",
        "#add patterns to ruler\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "#create the doc\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "PmhXFohxJt1E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for one very important reason. SpaCy’s EntityRuler cannot use RegEx to pattern match across tokens. The dash in the phone number throws off the EntityRuler. So, what are we to do in this scenario? Well, we have a few different options that we will explore in the next notebook. But before we get to that, let’s try and use RegEx to capture the phone number with no hyphen."
      ],
      "metadata": {
        "id": "XoKhDt3LKC1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample text\n",
        "text = \"This is a sample number 5555555.\"\n",
        "#Build upon the spaCy Small Model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "#Create the Ruler and Add it\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
        "patterns = [\n",
        "                {\n",
        "                    \"label\": \"PHONE_NUMBER\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"((\\d){5})\"}}\n",
        "                                                        ]\n",
        "                }\n",
        "            ]\n",
        "#add patterns to ruler\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "#create the doc\n",
        "doc = nlp(text)\n",
        "\n",
        "#extract entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6Sg0M9hKDbg",
        "outputId": "f7ddb4a6-6617-474e-fc58-9f6d9322d8e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5555555 PHONE_NUMBER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Working with Multi-Word Token Entities and RegEx in spaCy\n",
        "\n",
        "we can use spaCy’s Matcher to grab multi-word tokens, or tokens that span multiple tokens. The main problem with this, however, is that these multi-word tokens are not placed into the doc.ents. This means that we cannot access them the same way we would other entities\n",
        "\n",
        "\n",
        "## Extract Multi-Word Tokens\n",
        "\n",
        "First, we need to grab the multi-word tokens. In this notebook, we are going to try and grab a multi-word token. In this case, a person whose first name begins with Paul. In the RegEx below, we specify that we are looking for any string that starts with “Paul” and then is followed by a capitalized letter. We then tell it to grab the entire second word until the end of the word."
      ],
      "metadata": {
        "id": "C1Fqn9_MKY7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
        "\n",
        "pattern = r\"Paul [A-Z]\\w+\"\n",
        "\n",
        "matches = re.finditer(pattern, text)\n",
        "\n",
        "for match in matches:\n",
        "    print(match)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI-wEcyqJ96V",
        "outputId": "f0c39e53-8e53-4947-fbfe-f098b86fe3f3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
            "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have not grabbed the final “Paul” which is not followed by a last name. In this case, we are not interested in that Paul. Now that we know how to grab the multi-word tokens, we need to have a way to parse them in spaCy.\n",
        "\n",
        "## Reconstruct Spans\n",
        "\n",
        "This next stage is a bit more complicated, but works quite well once you understand the process. First, we need to import the libraries we will need. Note that we are also adding Span from spacy.tokens."
      ],
      "metadata": {
        "id": "sA2H9gSml2js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Span"
      ],
      "metadata": {
        "id": "DPSMdM2TSyDP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "pBxfKGFAmC3x"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Even though this part is unnecessary, it is good to do it here because \n",
        "# in other situations you will have entities. If you do, \n",
        "# you need to store them as a separate list to which we will append things.\n",
        "original_ents = list(doc.ents)"
      ],
      "metadata": {
        "id": "ztCj7HRxmG2f"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mwt_ents = []\n",
        "for match in re.finditer(pattern, doc.text):\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end)\n",
        "    if span is not None:\n",
        "        mwt_ents.append((span.start, span.end, span.text))\n",
        "\n",
        "mwt_ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8e9tiGamKsJ",
        "outputId": "e1d016db-3ff3-44bf-abd8-ffd3cc95fd93"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 2, 'Paul Newman'), (8, 10, 'Paul Hollywood')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nject the Spans into the `doc.ents`\n",
        "\n",
        "With that data, we can iterate over each entity and identify where it begins and ends in spaCy. Note, we are using the spaCy Span class. This allows us to create a span object and assign it a custom label. With this data, we can append each Span to `original_ents`."
      ],
      "metadata": {
        "id": "3f6_85V5m6E0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in mwt_ents:\n",
        "    start, end, name = ent\n",
        "    per_ent = Span(doc, start, end, label=\"PERSON\")\n",
        "    original_ents.append(per_ent)\n",
        "\n",
        "original_ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROmMU-Kwm0zC",
        "outputId": "8adfe947-1e61-4ec9-f297-af132145bd16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Paul Newman, Paul Hollywood]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.ents = original_ents"
      ],
      "metadata": {
        "id": "pVT0Rmp4nLBO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cykv2einPBw",
        "outputId": "cb3165af-10d8-4093-8995-f41f84a4fb51"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Newman PERSON\n",
            "Paul Hollywood PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Give priority to Longer Spans\n",
        "\n",
        "Sometimes, the situation is not so neat. Sometimes our custom RegEx entities will overlap with spaCy’s Entities"
      ],
      "metadata": {
        "id": "2wOmLIfGnaV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host.\"\n",
        "pattern = r\"Hollywood\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lunRsQ8PnTQe",
        "outputId": "13b26521-2da3-4cc0-d3a1-c9c73b4b6897"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Newman PERSON\n",
            "American NORP\n",
            "Paul Hollywood PERSON\n",
            "British NORP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s say that we create a new entity. Maybe words associated with Cinema. So, we want to classify Hollywood as a tag “CINEMA”. Now, in the above text, Hollywood is clearly associated with Paul Hollywood, but let’s imagine for a moment that it is not. Let’s try and run the same code as above. If we do, we notice that we get an error."
      ],
      "metadata": {
        "id": "bTNVVgcUnmSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mwt_ents = []\n",
        "original_ents = list(doc.ents)\n",
        "for match in re.finditer(pattern, doc.text):\n",
        "    print (match)\n",
        "    start, end = match.span()\n",
        "    span = doc.char_span(start, end)\n",
        "    if span is not None:\n",
        "        mwt_ents.append((span.start, span.end, span.text))\n",
        "        \n",
        "for ent in mwt_ents:\n",
        "    start, end, name = ent\n",
        "    per_ent = Span(doc, start, end, label=\"CINEMA\")\n",
        "    original_ents.append(per_ent)\n",
        "\n",
        "doc.ents = original_ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "TE0a6e-XnelA",
        "outputId": "3cd572d4-676f-4807-a04e-53d1d217cca8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(44, 53), match='Hollywood'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-425d356ded45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moriginal_ents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_ent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_ents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 9 which is included in more than one span in entities, blocked, missing or outside."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This error tells us that one of our tokens from the `finditer()` overlapped with one that our “ner” component found. This is a problem that can be rectified with spaCy’s `filter_spans`. This gives primacy to longer spans. Notice how we have allowed the Paul Hollywood entity to be a PERSON, rather than CINEMA. This is because Hollywood is shorter than Paul Hollywood."
      ],
      "metadata": {
        "id": "Ftrfb9KinvjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import filter_spans"
      ],
      "metadata": {
        "id": "VcsAEootns3x"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = filter_spans(original_ents)\n",
        "doc.ents = filtered\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M1INiE3n84j",
        "outputId": "d36afd74-2f75-4037-f8b6-07e927a7c34b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Newman PERSON\n",
            "American NORP\n",
            "Paul Hollywood PERSON\n",
            "British NORP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aBYDLf7Fpdcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}