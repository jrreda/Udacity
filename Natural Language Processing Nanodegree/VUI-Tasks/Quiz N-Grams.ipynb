{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed7dcc01-dfe3-4919-8662-3510315864d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# N-Grams\n",
    "\n",
    "An N-Gram is an ordered sequence of words. For example:\n",
    "\n",
    "![ngrams-numbers](images/ngrams-numbers.png)\n",
    "\n",
    "In the following series of quizes, you will work with 2-grams, or [bigrams](https://en.wikipedia.org/wiki/Bigram), as they are more commonly called.\n",
    "The objective is to create a function that calculates the probability that a particular sentence\n",
    "could occur in a corpus of text, based on the probabilities of its component bigrams. We'll do this in stages though:\n",
    "\n",
    "- Quiz 1 - Extract tokens and bigrams from a sentence\n",
    "- Quiz 2 - Calculate probabilities for bigrams\n",
    "- Quiz 3 - Calculate the log probability of a given sentence based on a corpus of text using bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cb726-7f8b-4418-9206-672559b8189f",
   "metadata": {},
   "source": [
    "## Quiz 1 - Extract tokens and bigrams from a sentence\n",
    "\n",
    "In the quiz below, write a function that returns a list of tokens and a list of bigrams for a given sentence. You will need to first break a sentence into words in a list, then add a `<s>` and `<s/>` token to the\n",
    "start and end of the list to represent the start and end of the sentence.\n",
    "\n",
    "Your final lists should be in the format shown above and called out in the function doc string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa6b25b-c96a-4247-9267-5796cc8d5144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<s>', 'the', 'old', 'man', 'spoke', 'to', 'me', '</s>'],\n",
       " [[('<s>', 'the'),\n",
       "   ('the', 'old'),\n",
       "   ('old', 'man'),\n",
       "   ('man', 'spoke'),\n",
       "   ('spoke', 'to'),\n",
       "   ('to', 'me'),\n",
       "   ('me', '</s>')]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    #TODO implement\n",
    "    sentence_tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    \n",
    "    sentence_bigrams = []\n",
    "    sentence_bigrams.append([(sentence_tokens[i], sentence_tokens[i+1]) for i in range(len(sentence_tokens)-1)])\n",
    "    return sentence_tokens, sentence_bigrams\n",
    "\n",
    "sentence_to_bigrams(test_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e7ace-8ea8-4741-9c93-05c45a2ee8c1",
   "metadata": {},
   "source": [
    "## Quiz 2 - Calculate Probabilities and Likelihoods with Bigrams\n",
    "\n",
    "Recall from a previous video that the probability of a series of words can be calculated from the chained probabilities of its history:\n",
    "\n",
    "![eqn-jointprob-words-in-sentence](images/eqn-jointprob-words-in-sentence.png)\n",
    "\n",
    "\n",
    "The probabilities of sequence occurrences in a large textual corpus can be calculated this way and used as a language model to add grammar and contectual knowledge to a speech\n",
    "recognition system. However, there is a prohibitively large number of calculations for all the possible sequences of varying length in a large textual corpus.\n",
    "\n",
    "To address this problem, we use the [Markov Assumption](https://en.wikipedia.org/wiki/Markov_property) to approximate a sequence probability with a shorter sequence:\n",
    "\n",
    "![eqn-markov-assumption-ngrams](images/eqn-markov-assumption-ngrams.png)\n",
    "\n",
    "In the bigram case, the equation reduces to a series of bigram probabilities multiplied together to find the approximate probability for a sentence. A concrete example:\n",
    "\n",
    "**`P(\"I Iove language models\") ≈ P(\"love\" ∣ \"I\") P(\"language\" ∣ \"love\") P(\"models\" ∣ \"language\") P(\"love\" ∣ \"I\") P(\"language\" ∣ \"love\") P(\"models\" ∣ \"language\")`**\n",
    "\n",
    "We can calculate the probabilities by using **[counts](https://docs.python.org/3.6/library/collections.html#collections.Counter)** of the bigramsand individual tokens. The counts are represented below with the `c()` operator:\n",
    "\n",
    "![eqn-bigram-mle](images/eqn-bigram-mle.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715ebe8a-9089-47ae-a568-e25ee15a40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/54962539/how-to-get-the-probability-of-bigrams-in-a-text-of-sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098a408b-4bb6-4d6a-9232-0bbfacc860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utils\n",
    "\n",
    "def bigrams_from_transcript(filename):\n",
    "    \"\"\"\n",
    "    read a file of sentences, adding start '<s>' and stop '</s>' tags; Tokenize it into a list of lower case words\n",
    "    and bigrams\n",
    "    :param filename: string \n",
    "        filename: path to a text file consisting of lines of non-puncuated text; assume one sentence per line\n",
    "    :return: list, list\n",
    "        tokens: ordered list of words found in the file\n",
    "        bigrams: a list of ordered two-word tuples found in the file\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    bigrams = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            line_tokens, line_bigrams = sentence_to_bigrams(line)\n",
    "            tokens = tokens + line_tokens\n",
    "            bigrams = bigrams + line_bigrams\n",
    "    return tokens, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88aa9c94-16a8-4abc-94a8-85bf076fd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def sample_run():\n",
    "    # sample usage by test code (this definition not actually run for the quiz)\n",
    "    tokens, bigrams = bigrams_from_transcript('transcripts.txt')\n",
    "    bg_dict = bigram_mle(tokens, bigrams)\n",
    "    print(bg_dict)\n",
    "\n",
    "\n",
    "def bigram_mle(tokens, bigrams):\n",
    "    \"\"\"\n",
    "    provide a dictionary of probabilities for all bigrams in a corpus of text\n",
    "    the calculation is based on maximum likelihood estimation and does not include\n",
    "    any smoothing. \n",
    "    A tag '<unk>' has been added for unknown probabilities.\n",
    "    :param tokens: list\n",
    "        tokens: list of all tokens in the corpus\n",
    "    :param bigrams: list\n",
    "        bigrams: list of all two word tuples in the corpus\n",
    "    :return: dict\n",
    "        bg_mle_dict: a dictionary of bigrams:\n",
    "            key: tuple of two bigram words, in order OR <unk> key\n",
    "            value: float probability\n",
    "\n",
    "    \"\"\"\n",
    "    bg_mle_dict = {}\n",
    "    bg_mle_dict['<unk>'] = 0.\n",
    "    \n",
    "    #TODO implement\n",
    "    token_counter = Counter(tokens)\n",
    "    bigram_raw = []\n",
    "    for bigram in bigrams:\n",
    "        bigram_raw += bigram\n",
    "    bigram_raw_counts = Counter(bigram_raw)\n",
    "    \n",
    "    for bg in bigram_raw_counts:\n",
    "        bg_mle_dict[bg] = bigram_raw_counts[bg] / token_counter[bg[0]]\n",
    "    return bg_mle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db75cdbe-74b2-44ac-a4b5-0f99b8486e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens, bigrams = bigrams_from_transcript('transcripts.txt')\n",
    "# bigram_mle(tokens, bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09106532-3608-450c-ba91-61855e3b3885",
   "metadata": {},
   "source": [
    "## Quiz 3 - Calculate the log probability of a given sentence based on a corpus of text using bigrams\n",
    "\n",
    "There are still a couple of problems to sort out before we use the bigram probability dictionary to calculate the probabilities of new sentences:\n",
    "\n",
    "1. Some possible combinations may not exist in our probability dictionary but are still possible. We don't want to multiply in a probability of 0 just because our original corpus was deficient. This is solved through \"smoothing\". There are a number of methods for this, but a simple one is the [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) with the \"add-one\" estimate where VV is the size of the vocabulary for the corpus, i.e. the number of unique tokens:\n",
    "\n",
    "\n",
    "![eqn-addone-bigram-smoothing](images/eqn-addone-bigram-smoothing.png)\n",
    "\n",
    "2. Repeated multiplications of small probabilities can cause underflow problems in computers when the values become to small. To solve this, we will calculate all probabilities in log space: \n",
    "\n",
    "    `**log(p1 × p2 × p3 × p4) = log p1 + log p2 + log p3 + log p4**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ee411c-bec2-4db5-9b01-e4bececb7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utils\n",
    "import numpy as np\n",
    "\n",
    "def bigram_add1_logs(transcript_file):\n",
    "    \"\"\"\n",
    "    provide a smoothed log probability dictionary based on a transcript\n",
    "    :param transcript_file: string\n",
    "        transcript_file is the path filename containing unpunctuated text sentences\n",
    "    :return: dict\n",
    "        bg_add1_log_dict: dictionary of smoothed bigrams log probabilities including\n",
    "        tags: <s>: start of sentence, </s>: end of sentence, <unk>: unknown placeholder probability\n",
    "    \"\"\"\n",
    "\n",
    "    tokens, bigrams = bigrams_from_transcript(transcript_file)\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    bigram_raw = []\n",
    "    for bigram in bigrams:\n",
    "        bigram_raw += bigram\n",
    "    bigram_counts = Counter(bigram_raw)\n",
    "    \n",
    "    vocab_count = len(token_counts)\n",
    "\n",
    "    bg_addone_dict = {}\n",
    "    for bg in bigram_counts:\n",
    "        bg_addone_dict[bg] = np.log((bigram_counts[bg] + 1.) / (token_counts[bg[0]] + vocab_count))\n",
    "    bg_addone_dict['<unk>'] = np.log(1. / vocab_count)\n",
    "    return bg_addone_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "961ffa91-1ff1-47e5-b5a9-f7dd1d1e8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_run():\n",
    "    # sample usage by test code (this definition not actually run for the quiz)\n",
    "    bigram_log_dict = bigram_add1_logs('transcripts.txt')\n",
    "    for sentence in test_sentences:\n",
    "        print(f'*** \"{sentence}\"')\n",
    "        print(log_prob_of_sentence(sentence, bigram_log_dict))\n",
    "\n",
    "def log_prob_of_sentence(sentence):\n",
    "    total_log_prob = 0.\n",
    "\n",
    "    # TODO implement\n",
    "    # get the sentence bigrams with sentence_to_bigrams\n",
    "    # look up the bigrams from the sentence in the bigram_log_dict\n",
    "    # add all the log probabilities together\n",
    "    # if a word doesn't exist, be sure to use the value of the '<unk>' lookup instead\n",
    "    tokens, bigrams = sentence_to_bigrams(sentence)\n",
    "    bigram_log_dict = bigram_mle(tokens, bigrams)\n",
    "\n",
    "    for bg in bigrams:\n",
    "        for i in range(len(bg)):\n",
    "            if bg[i] in bigram_log_dict:\n",
    "                total_log_prob += bigram_log_dict[bg[i]]\n",
    "            else:\n",
    "                total_log_prob += bigram_log_dict['<unk>']\n",
    "                \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d66ed056-69eb-491d-9e4a-f7f6a52db619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the old man spoke to me\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_sentences[0])\n",
    "log_prob_of_sentence(test_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1e4b87c-2ded-4362-b187-edb751fa15f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me to spoke man old the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_sentences[1])\n",
    "log_prob_of_sentence(test_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2db3133-1c2e-4a37-a4db-1bafb20b48cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old man me old man me\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_sentences[2])\n",
    "log_prob_of_sentence(test_sentences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60715fe7-7e11-42fa-9962-bb9066a2a444",
   "metadata": {},
   "source": [
    "**more likely sentences yield higher values for the log probabilities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c5d62-dbe7-4a24-b1bd-bb3a908f8fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
